<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DeformableDETR | ZQLog</title>
<meta name=keywords content="深度学习,Transformer,目标检测"><meta name=description content="DeformableDETR 1.DETR 1.1 简述 Detr将目标检测任务看作是集合预测问题，在每次将所有的object都做出预测，之后与GroundTruth进行二分图匹配，每一个object预测结果都会对应一个种类与位置(当然其中包含背景种类)；并且使用transformer来构建模型，对所有的objectquery来并行预测。
相比较以往的目标检测模型，DETR主要有以下几个创新点：
将目标检测任务看作是集合预测问题，并为此设计了损失函数，得到固定数量的object预测之后与真实值进行二分匹配，从而不需要以往的去重工作（如NMS） 使用transformer来并行预测与建模，不再需要先验知识(如anchor的设计) 总之，detr是首个真正的端到端目标检测模型，也是将transformer运用到目标检测任务中的首次尝试。
1.2损失函数（Object detection set prediction loss） DETR每次得出固定数量$N$ 的预测结果，当然$N$ 需要大于在图片中出现的物体数量，之后得出这些预测结果与真实值之间的最佳二分匹配，得出最佳匹配的方法表示如下：
其中，$y$ 代表所预测目标的真实值，当然其中包括物体的种类与位置；$\hat y={\hat y_i}{i=1}^{N}$ 表示对每个objectquery得到的预测结果,$\sigma(i)$ 则表示对于第$i$ 个预测结果所匹配到的groundtruth的索引，$\ell{match}(y_i,\hat y_{\sigma(i)})$ 表示二者之间的匹配成本。我们最终需要的使匹配成本最小的$\sigma$ 。当然这里可以使用匈牙利算法。
1.2.1匹配成本 这里的匹配成本同时考虑了物体的种类与位置，即$\hat y = (c_i,b_i)$ ,其中$c_i$表示物体种类，$b_i \in [0,1]^4$ 为包含ground box中心坐标与长、宽的向量。对于索引$\sigma(i)$ 我们将该索引对应的预测结果分类为$c_i$ 的概率表示为$p_{\sigma(i)}(c_i)$ ,将其预测的box表示为$\hat b_{\sigma (i)}$ ，因此可以将匹配成本函数定义为：$L_{match}(y_i, \hat y_{\sigma(i)}) =−1_{c_i\not=\emptyset}\hat p_{\sigma(i)}(c_i) + 1_{c_i\not=\emptyset} L_{box}(b_i, \hat b_{\sigma_i}(i))$ 。这样的过程与以往的目标检测算法中为anchor或者proposal分配真实值的工作效果相同，当然区别在与我们得到的是一一对应的匹配，不存在重复的问题。
1.2.2 标记框损失(Bounding box loss) 因为本模型是直接做出对于box的预测的，所以会遇到不同尺度下损失的相对缩放。常用的$l_1$ 损失会跟随预测框的大小产生缩放，即使这两种预测框的错误程度是相同的。而IoU损失的尺度是不变的，因此这里将$l_1$ 损失与IoU损失做线性组合，成为我们需要的boxloss，如下：
当然，其中$\lambda_{iou}$ 与$\lambda_{L_1}$ 都是超参数。
1.2.3 损失函数 之后就可以定义匹配的损失函数如下：
1.3 网络结构 (DETR architecture) 先上图"><meta name=author content="作者：zql"><link rel=canonical href=https://zqlqy.github.io/posts/deformabledetr/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://zqlqy.github.io/Icon_W.jpg><link rel=icon type=image/png sizes=16x16 href=https://zqlqy.github.io/Icon_W.jpg><link rel=icon type=image/png sizes=32x32 href=https://zqlqy.github.io/Icon_W.jpg><link rel=apple-touch-icon href=https://zqlqy.github.io/Icon_W.jpg><link rel=mask-icon href=https://zqlqy.github.io/Icon_W.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://zqlqy.github.io/posts/deformabledetr/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link href="https://fonts.cdnfonts.com/css/code-new-roman?styles=20776" rel=stylesheet><style>@import 'https://fonts.cdnfonts.com/css/code-new-roman'</style><meta property="og:title" content="DeformableDETR"><meta property="og:description" content="DeformableDETR 1.DETR 1.1 简述 Detr将目标检测任务看作是集合预测问题，在每次将所有的object都做出预测，之后与GroundTruth进行二分图匹配，每一个object预测结果都会对应一个种类与位置(当然其中包含背景种类)；并且使用transformer来构建模型，对所有的objectquery来并行预测。
相比较以往的目标检测模型，DETR主要有以下几个创新点：
将目标检测任务看作是集合预测问题，并为此设计了损失函数，得到固定数量的object预测之后与真实值进行二分匹配，从而不需要以往的去重工作（如NMS） 使用transformer来并行预测与建模，不再需要先验知识(如anchor的设计) 总之，detr是首个真正的端到端目标检测模型，也是将transformer运用到目标检测任务中的首次尝试。
1.2损失函数（Object detection set prediction loss） DETR每次得出固定数量$N$ 的预测结果，当然$N$ 需要大于在图片中出现的物体数量，之后得出这些预测结果与真实值之间的最佳二分匹配，得出最佳匹配的方法表示如下：
其中，$y$ 代表所预测目标的真实值，当然其中包括物体的种类与位置；$\hat y={\hat y_i}{i=1}^{N}$ 表示对每个objectquery得到的预测结果,$\sigma(i)$ 则表示对于第$i$ 个预测结果所匹配到的groundtruth的索引，$\ell{match}(y_i,\hat y_{\sigma(i)})$ 表示二者之间的匹配成本。我们最终需要的使匹配成本最小的$\sigma$ 。当然这里可以使用匈牙利算法。
1.2.1匹配成本 这里的匹配成本同时考虑了物体的种类与位置，即$\hat y = (c_i,b_i)$ ,其中$c_i$表示物体种类，$b_i \in [0,1]^4$ 为包含ground box中心坐标与长、宽的向量。对于索引$\sigma(i)$ 我们将该索引对应的预测结果分类为$c_i$ 的概率表示为$p_{\sigma(i)}(c_i)$ ,将其预测的box表示为$\hat b_{\sigma (i)}$ ，因此可以将匹配成本函数定义为：$L_{match}(y_i, \hat y_{\sigma(i)}) =−1_{c_i\not=\emptyset}\hat p_{\sigma(i)}(c_i) + 1_{c_i\not=\emptyset} L_{box}(b_i, \hat b_{\sigma_i}(i))$ 。这样的过程与以往的目标检测算法中为anchor或者proposal分配真实值的工作效果相同，当然区别在与我们得到的是一一对应的匹配，不存在重复的问题。
1.2.2 标记框损失(Bounding box loss) 因为本模型是直接做出对于box的预测的，所以会遇到不同尺度下损失的相对缩放。常用的$l_1$ 损失会跟随预测框的大小产生缩放，即使这两种预测框的错误程度是相同的。而IoU损失的尺度是不变的，因此这里将$l_1$ 损失与IoU损失做线性组合，成为我们需要的boxloss，如下：
当然，其中$\lambda_{iou}$ 与$\lambda_{L_1}$ 都是超参数。
1.2.3 损失函数 之后就可以定义匹配的损失函数如下：
1.3 网络结构 (DETR architecture) 先上图"><meta property="og:type" content="article"><meta property="og:url" content="https://zqlqy.github.io/posts/deformabledetr/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-19T11:30:13+08:00"><meta property="article:modified_time" content="2024-05-19T11:30:13+08:00"><meta property="og:site_name" content="ZQLog"><meta name=twitter:card content="summary"><meta name=twitter:title content="DeformableDETR"><meta name=twitter:description content="DeformableDETR 1.DETR 1.1 简述 Detr将目标检测任务看作是集合预测问题，在每次将所有的object都做出预测，之后与GroundTruth进行二分图匹配，每一个object预测结果都会对应一个种类与位置(当然其中包含背景种类)；并且使用transformer来构建模型，对所有的objectquery来并行预测。
相比较以往的目标检测模型，DETR主要有以下几个创新点：
将目标检测任务看作是集合预测问题，并为此设计了损失函数，得到固定数量的object预测之后与真实值进行二分匹配，从而不需要以往的去重工作（如NMS） 使用transformer来并行预测与建模，不再需要先验知识(如anchor的设计) 总之，detr是首个真正的端到端目标检测模型，也是将transformer运用到目标检测任务中的首次尝试。
1.2损失函数（Object detection set prediction loss） DETR每次得出固定数量$N$ 的预测结果，当然$N$ 需要大于在图片中出现的物体数量，之后得出这些预测结果与真实值之间的最佳二分匹配，得出最佳匹配的方法表示如下：
其中，$y$ 代表所预测目标的真实值，当然其中包括物体的种类与位置；$\hat y={\hat y_i}{i=1}^{N}$ 表示对每个objectquery得到的预测结果,$\sigma(i)$ 则表示对于第$i$ 个预测结果所匹配到的groundtruth的索引，$\ell{match}(y_i,\hat y_{\sigma(i)})$ 表示二者之间的匹配成本。我们最终需要的使匹配成本最小的$\sigma$ 。当然这里可以使用匈牙利算法。
1.2.1匹配成本 这里的匹配成本同时考虑了物体的种类与位置，即$\hat y = (c_i,b_i)$ ,其中$c_i$表示物体种类，$b_i \in [0,1]^4$ 为包含ground box中心坐标与长、宽的向量。对于索引$\sigma(i)$ 我们将该索引对应的预测结果分类为$c_i$ 的概率表示为$p_{\sigma(i)}(c_i)$ ,将其预测的box表示为$\hat b_{\sigma (i)}$ ，因此可以将匹配成本函数定义为：$L_{match}(y_i, \hat y_{\sigma(i)}) =−1_{c_i\not=\emptyset}\hat p_{\sigma(i)}(c_i) + 1_{c_i\not=\emptyset} L_{box}(b_i, \hat b_{\sigma_i}(i))$ 。这样的过程与以往的目标检测算法中为anchor或者proposal分配真实值的工作效果相同，当然区别在与我们得到的是一一对应的匹配，不存在重复的问题。
1.2.2 标记框损失(Bounding box loss) 因为本模型是直接做出对于box的预测的，所以会遇到不同尺度下损失的相对缩放。常用的$l_1$ 损失会跟随预测框的大小产生缩放，即使这两种预测框的错误程度是相同的。而IoU损失的尺度是不变的，因此这里将$l_1$ 损失与IoU损失做线性组合，成为我们需要的boxloss，如下：
当然，其中$\lambda_{iou}$ 与$\lambda_{L_1}$ 都是超参数。
1.2.3 损失函数 之后就可以定义匹配的损失函数如下：
1.3 网络结构 (DETR architecture) 先上图"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zqlqy.github.io/posts/"},{"@type":"ListItem","position":2,"name":"DeformableDETR","item":"https://zqlqy.github.io/posts/deformabledetr/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DeformableDETR","name":"DeformableDETR","description":"DeformableDETR 1.DETR 1.1 简述 Detr将目标检测任务看作是集合预测问题，在每次将所有的object都做出预测，之后与GroundTruth进行二分图匹配，每一个object预测结果都会对应一个种类与位置(当然其中包含背景种类)；并且使用transformer来构建模型，对所有的objectquery来并行预测。\n相比较以往的目标检测模型，DETR主要有以下几个创新点：\n将目标检测任务看作是集合预测问题，并为此设计了损失函数，得到固定数量的object预测之后与真实值进行二分匹配，从而不需要以往的去重工作（如NMS） 使用transformer来并行预测与建模，不再需要先验知识(如anchor的设计) 总之，detr是首个真正的端到端目标检测模型，也是将transformer运用到目标检测任务中的首次尝试。\n1.2损失函数（Object detection set prediction loss） DETR每次得出固定数量$N$ 的预测结果，当然$N$ 需要大于在图片中出现的物体数量，之后得出这些预测结果与真实值之间的最佳二分匹配，得出最佳匹配的方法表示如下：\n其中，$y$ 代表所预测目标的真实值，当然其中包括物体的种类与位置；$\\hat y={\\hat y_i}{i=1}^{N}$ 表示对每个objectquery得到的预测结果,$\\sigma(i)$ 则表示对于第$i$ 个预测结果所匹配到的groundtruth的索引，$\\ell{match}(y_i,\\hat y_{\\sigma(i)})$ 表示二者之间的匹配成本。我们最终需要的使匹配成本最小的$\\sigma$ 。当然这里可以使用匈牙利算法。\n1.2.1匹配成本 这里的匹配成本同时考虑了物体的种类与位置，即$\\hat y = (c_i,b_i)$ ,其中$c_i$表示物体种类，$b_i \\in [0,1]^4$ 为包含ground box中心坐标与长、宽的向量。对于索引$\\sigma(i)$ 我们将该索引对应的预测结果分类为$c_i$ 的概率表示为$p_{\\sigma(i)}(c_i)$ ,将其预测的box表示为$\\hat b_{\\sigma (i)}$ ，因此可以将匹配成本函数定义为：$L_{match}(y_i, \\hat y_{\\sigma(i)}) =−1_{c_i\\not=\\emptyset}\\hat p_{\\sigma(i)}(c_i) + 1_{c_i\\not=\\emptyset} L_{box}(b_i, \\hat b_{\\sigma_i}(i))$ 。这样的过程与以往的目标检测算法中为anchor或者proposal分配真实值的工作效果相同，当然区别在与我们得到的是一一对应的匹配，不存在重复的问题。\n1.2.2 标记框损失(Bounding box loss) 因为本模型是直接做出对于box的预测的，所以会遇到不同尺度下损失的相对缩放。常用的$l_1$ 损失会跟随预测框的大小产生缩放，即使这两种预测框的错误程度是相同的。而IoU损失的尺度是不变的，因此这里将$l_1$ 损失与IoU损失做线性组合，成为我们需要的boxloss，如下：\n当然，其中$\\lambda_{iou}$ 与$\\lambda_{L_1}$ 都是超参数。\n1.2.3 损失函数 之后就可以定义匹配的损失函数如下：\n1.3 网络结构 (DETR architecture) 先上图","keywords":["深度学习","Transformer","目标检测"],"articleBody":"DeformableDETR 1.DETR 1.1 简述 Detr将目标检测任务看作是集合预测问题，在每次将所有的object都做出预测，之后与GroundTruth进行二分图匹配，每一个object预测结果都会对应一个种类与位置(当然其中包含背景种类)；并且使用transformer来构建模型，对所有的objectquery来并行预测。\n相比较以往的目标检测模型，DETR主要有以下几个创新点：\n将目标检测任务看作是集合预测问题，并为此设计了损失函数，得到固定数量的object预测之后与真实值进行二分匹配，从而不需要以往的去重工作（如NMS） 使用transformer来并行预测与建模，不再需要先验知识(如anchor的设计) 总之，detr是首个真正的端到端目标检测模型，也是将transformer运用到目标检测任务中的首次尝试。\n1.2损失函数（Object detection set prediction loss） DETR每次得出固定数量$N$ 的预测结果，当然$N$ 需要大于在图片中出现的物体数量，之后得出这些预测结果与真实值之间的最佳二分匹配，得出最佳匹配的方法表示如下：\n其中，$y$ 代表所预测目标的真实值，当然其中包括物体的种类与位置；$\\hat y={\\hat y_i}{i=1}^{N}$ 表示对每个objectquery得到的预测结果,$\\sigma(i)$ 则表示对于第$i$ 个预测结果所匹配到的groundtruth的索引，$\\ell{match}(y_i,\\hat y_{\\sigma(i)})$ 表示二者之间的匹配成本。我们最终需要的使匹配成本最小的$\\sigma$ 。当然这里可以使用匈牙利算法。\n1.2.1匹配成本 这里的匹配成本同时考虑了物体的种类与位置，即$\\hat y = (c_i,b_i)$ ,其中$c_i$表示物体种类，$b_i \\in [0,1]^4$ 为包含ground box中心坐标与长、宽的向量。对于索引$\\sigma(i)$ 我们将该索引对应的预测结果分类为$c_i$ 的概率表示为$p_{\\sigma(i)}(c_i)$ ,将其预测的box表示为$\\hat b_{\\sigma (i)}$ ，因此可以将匹配成本函数定义为：$L_{match}(y_i, \\hat y_{\\sigma(i)}) =−1_{c_i\\not=\\emptyset}\\hat p_{\\sigma(i)}(c_i) + 1_{c_i\\not=\\emptyset} L_{box}(b_i, \\hat b_{\\sigma_i}(i))$ 。这样的过程与以往的目标检测算法中为anchor或者proposal分配真实值的工作效果相同，当然区别在与我们得到的是一一对应的匹配，不存在重复的问题。\n1.2.2 标记框损失(Bounding box loss) 因为本模型是直接做出对于box的预测的，所以会遇到不同尺度下损失的相对缩放。常用的$l_1$ 损失会跟随预测框的大小产生缩放，即使这两种预测框的错误程度是相同的。而IoU损失的尺度是不变的，因此这里将$l_1$ 损失与IoU损失做线性组合，成为我们需要的boxloss，如下：\n当然，其中$\\lambda_{iou}$ 与$\\lambda_{L_1}$ 都是超参数。\n1.2.3 损失函数 之后就可以定义匹配的损失函数如下：\n1.3 网络结构 (DETR architecture) 先上图\n1.3.1 Backbone 即:使用CNN从原始三通道图像$x_{img}\\in R^{3\\times H_0 \\times W_0}$ 抽取出特征图$f \\in R^{d \\times h \\times w}$ 。在原文中，$C=2048$,$h,w=h_0/32,w_0/32$\n1.3.2 Transformer Encoder 在编码器中，首先使用$1 \\times 1$卷积核，将特征图f 的通道数从C降为d，创建特征图$z_0 \\in R^{d \\times h \\times w}$。因为transformer的编码器需要的是序列化的输入，因此再将$z_0$展平为$d \\times hw$的序列，这个时候每个像素点可以类比为序列中的一个word embedding；之后按照transformer的流程，再加入位置编码，不同的是，每一次进行注意力机制之前都需要加入一次位置编码，不同于transformer的只加入一次。\n之后即可进行多头自注意力机制与前馈网络(FFN)，与transformer一致。\n1.3.3 Transformer Decoder 解码器同样与transformer非常相近，输入为N个objectqurey,长度为d，这些作为query进行多头注意力与自注意力机制。不同点在于，原transformer在做预测的时候依然是串行进行的，即每次根据之前的输入预测下一个元素；而本文中则是对所有的object query并行做出预测，同时产生$N$ 个预测结果，当然这些预测结果都是各不相同的。\n多次提到的object query,实际上就是一种可学习的位置编码，当然也同样在每层中的注意力机制加入。\n得益于attention，我们可以同时看到整张图片，从而做出充分考虑上下文内容的预测；但是超大的感受野也为detr埋下了隐患。\n1.3.4 FFNs 在decoder中，N个object query会产生N个输出，而最终的预测结果是由FFN加工完成的。对于decoder输出的embedding,FFN用一个具有ReLU激活函数和d维隐藏层的3层感知机，来根据输出得到预测框的相对于原图归一化后中心点坐标、长和宽；再通过具有softmax的线性映射将输出映射为预测类别。\n1.3.5 辅助 在训练时，还用到了辅助的损失函数，来帮助模型正确输出每个种类的物体的数量。具体为在每层decoder中都使用FFN与匈牙利损失。所有的FFN都具有相同的参数。\n1.4 DETR的局限性 detr成功得将transformer运用到目标检测领域，代替了此前在目标检测中的anchor标准和筛选处理，成为第一个完全端到端目标检测器，但是它仍然存在一些问题：\n由于transformer的特性(注意力机制能同时看到所有字)导致计算开销大，收敛慢 处理图像的分辨率有限制，在小目标识别方面有提升空间(但只是相对于DETR来说) 基于为了改进以上两点，本文提出了DeformalbeDETR\n2. DCN 接下来需要了解可变形卷积的基本思想。\n2.1普通卷积 普通卷积运算公式如下：\n其中，$p_0$代表卷积核心的坐标，$p_n$ 代表卷积核感受野中需要纳入考量的单元相对于核心的偏移量。\n可入下图所示：\n在普通卷积下，相对于核心的偏移都是固定的，这也限制了卷积的感受野，对于学习到较远距离元素的关系计算开销很大。\n2.2可变形卷积 计算公式：\n可变形卷积中进一步加入了$\\triangle p_n$ ，是通过一层卷积的到的值，学习方法：\n$\\triangle p_n$是通过在特征图上增加一个卷积层来得到的。这个卷积核的尺寸与可变形卷积的尺寸一致。输出的offset fields有着与输入特征图一样的尺寸，通道维度2N与N个2D的offsets相关联\n这里offsetfield的channel数为18（9个点的偏移量，每个量2个参数）\n$\\triangle p_n$代表偏移量的偏移量，如下图所示：\n当然$\\triangle p_n$ 通常是小数，为了计算参与卷积运算的点的值，这里会选择距编译点最近的四个点进行双线性差值，其结果参与卷积运算。\n因此可变形卷积可如下表示：\n3.DeformableDETR 为了解决DETR中的两个问题，该论文有以下两个创新点：\n3.1可变性注意力模块(Deformable Attention Module) 针对注意力机制会处理所有位置的token,参考可变形卷积，不论特征图有多大，对于每个query只对有限的几个key应用注意力机制(在代码中这个采样的数量是4)，减少了运算开销，从而缓解收敛满和图像分辨率限制的问题。\n对于特征图$x \\in R^{C\\times H \\times W}$ ，$z_q$ 表示特征图中的一个token,$p_q$ 表示该token在特征图中的二维坐标(类比卷积)，则可变形注意力机制可以表示为：\n其中，m标准多头注意力的序号；k表示采样得到的key的序号；$\\Delta P_{mqk}$ 表示对于key采样的偏移，由query $z_q$ 线性映射获得；由$p_q$ 表示原本应当采样的坐标(这里和可变性卷积稍有出入)；因此$x(p_q+\\Delta p_{mqk})$ 表示在特征图中采样得到的token的二维坐标（当然这里也用到了双线性差值），用该坐标位置的token对应的key(需要经过线性映射)的key与value,与query进行注意力机制，而后得到对应的注意力权重$A_{mqk}$（当然原文中$A_{mqk}$ 也是由$z_q$ 线性映射后softmax得到的，是注意力的变体，不是传统的注意力机制）；最后再乘以权重参数$W_m^{’}$ (原文中这个是单位矩阵，费解)，最后再乘以不同注意力头分配的权重，得到注意力机制的结果。\n3.2多尺度可变性注意力模块(Multi-scale Deformable Attention Module) 以上提到的注意力机制可以很自然地应用于多尺度特征图中，公式如下：\n其中，m表示注意力头序号，k表示采样点序号，l表示特征图的尺度等级。\n特别需要说明的是，为了在不同尺度(当然这里可以理解为分辨率)下应用这个可变性注意力机制，我们需要进行采样时坐标的处理，因此这里将采样点二维坐标进行归一化$\\hat p_q \\in [0,1]^2$,则在不同特征图尺度下可以找到同一位置的点。之后为了在不同尺度下取的指定位置的$z_q$ 值，再通过$\\phi_l()$ 函数将$p_q$ 映射回原尺度得到坐标。\n3.3 Deformable Transformer Encoder 在编码器中，从ResNet的$C_3$ 到$C_5$ 阶段的输出中提取四个等级尺度$(L=4)$的特征图，$C_l$ 阶段的特征图比原图低 $2^l$ 倍，最低分辨率特征图是$C_6$,也就是$C_5$ 再通过步长为2的$3\\times3$ 卷积得到。编码器的输出是和输入相同分辨率的多尺度特征图，key和query元素都是多尺度特征图中的像素。对于每一个query,参考点就是它本身。\n为了表示尺度信息，类比transformer中的position embedding,在这里也加入了scale-level embedding,这个embedding是通过网络训练而成的。\n3.4 Deformable Transformer Decoder 实际是在学习检测框的特征\n解码器中有交叉注意力与自注意力，两种注意力的query都是object。在交叉注意力中，key来自编码器，object query从特征图中抽取特征。在自注意中，object query互相作用，这时key为object query本身。当然在这里只在交叉注意力之中，应用deformable attention，自注意力不变。对于每一个object query,参考点的坐标是根据object embedding通过一层可学习的线性映射得到的。\n因为多尺度可变形卷积是依据参考点来提取特征的，所以我们用检测头预测boundingbox作为相对于参考点的坐标偏移，参考点作为最初预测框的中心。因此，解码器的注意力机制会与预测框有很大的相关性。\n4.BoundingBox回归损失 4.1 L1损失 L1损失也即平均绝对误差(MAE)，顾名思义公式如下：\n$$ L1(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n其曲线分布如下：\n特点：\n导数为常量，不会有梯度爆炸问题； 对于离群点的惩罚是是固定的； 但是：\n在0处不可导 正因为导数为常量，对于较小的损失值也具有较大的梯度，缺乏自适应性 4.2 L2损失 也称均方误差(MSE)，一目了然公式如下：\n$$ L2(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n其曲线如下：\n特点：\n出处连续，处处可导； 并且随着误差值减小，梯度也会减小，利于收敛 但是：\n当$y-f(x)$ 差值大于1时，平方项会放大误差，对离群点惩罚比较大 4.3 Smooth L1 即以上两种损失的联合，在误差小于1时为L2损失，大于1时为L1损失\n公式如下：\n$$ \\text{SmoothL1}(y, \\hat{y}) = \\begin{cases} 0.5 * (y - \\hat{y})^2 \u0026 \\text{if } |y - \\hat{y}| \u003c 1 \\ |y - \\hat{y}| - 0.5 \u0026 \\text{otherwise} \\end{cases} $$\n曲线如下：\n特点：\n当误差过大，梯度不会过大 当误差比较小，梯度也会相应减小 实际上就是联合了以上两种函数的优点。\n5. IoU损失 5.1 IoU**(Intersection over Union)** 用于衡量模型预测的区域与真实区域之间的重叠程度\n即计算两个区域的交集面积与它们的并集面积之比。\nIoU损失的计算公式如下所示：\n$$ IoU = \\frac{{\\text{Intersection Area}}}{{\\text{Union Area}}} = \\frac{{\\text{TP}}}{{\\text{TP + FP + FN}}} $$\n其中：\nTP（True Positive）：真正例，模型正确预测的正样本数量。 FP（False Positive）：假正例，模型错误预测的正样本数量。 FN（False Negative）：假负例，模型未能正确预测的正样本数量。 实际上也可简单理解为交并比：\n$$ IoU = \\frac{{|A \\cap B|}}{{|A \\cup B|}} $$\nIoU损失越小，表示模型的预测与真实情况的重叠程度越大。\n由于IoU为比值形式，所以它具有一个很好的特性是，对尺度(scale)不敏感\n5.2 GIoU**(Generalized Intersection over Union)** 由于BBox中的回归损失对scale比较敏感，并且其优化与IoU损失的优化不是等价的，因此对IoU损失进行改进，在计算目标检测框的重叠程度时考虑了检测框的整体位置和大小关系，不仅仅是交并关系，从而设置为回归损失：\n$$ GIoU = IoU - \\frac{{|C- (A \\cup B)|}}{{|C|}} $$\n其中：\n$|C - (A \\cup B)|$ 表示外部包围框 C 减去 A和 B的并集的大小（即不相交的部分的大小）。 |C| 示外部包围框 C 的大小，同时包含了预测框和真实框的最小框的面积。 GIoU 将 IoU 的值减去了外部不相交部分与外部包围框的比例，不仅关注重叠区域，还关注非重叠区域，从而考虑了检测框的整体位置关系,能更好的反映两者的重合度。使得 GIoU 更能够准确地反映检测框的匹配情况。\n","wordCount":"373","inLanguage":"en","datePublished":"2024-05-19T11:30:13+08:00","dateModified":"2024-05-19T11:30:13+08:00","author":{"@type":"Person","name":"作者：zql"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zqlqy.github.io/posts/deformabledetr/"},"publisher":{"@type":"Organization","name":"ZQLog","logo":{"@type":"ImageObject","url":"https://zqlqy.github.io/Icon_W.jpg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://zqlqy.github.io/ accesskey=h title="ZQLog (Alt + H)"><img src=https://zqlqy.github.io/Icon_W.jpg alt aria-label=logo height=35>ZQLog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://zqlqy.github.io/posts/ title=posts><span>posts</span></a></li><li><a href=https://zqlqy.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://zqlqy.github.io/archives/ title=archives><span>archives</span></a></li><li><a href=https://zqlqy.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://zqlqy.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://zqlqy.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">DeformableDETR</h1><div class=post-meta><span title='2024-05-19 11:30:13 +0800 CST'>2024-05-19</span>&nbsp;·&nbsp;373 words&nbsp;·&nbsp;作者：zql</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#deformabledetr aria-label=DeformableDETR>DeformableDETR</a></li><li><a href=#1detr aria-label=1.DETR>1.DETR</a><ul><li><a href=#11-%e7%ae%80%e8%bf%b0 aria-label="1.1 简述">1.1 简述</a></li><li><a href=#12%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0object-detection-set-prediction-loss aria-label="1.2损失函数（Object detection set prediction loss）">1.2损失函数（Object detection set prediction loss）</a><ul><li><a href=#121%e5%8c%b9%e9%85%8d%e6%88%90%e6%9c%ac aria-label=1.2.1匹配成本>1.2.1匹配成本</a></li><li><a href=#122-%e6%a0%87%e8%ae%b0%e6%a1%86%e6%8d%9f%e5%a4%b1bounding-box-loss aria-label="1.2.2 标记框损失(Bounding box loss)">1.2.2 标记框损失(Bounding box loss)</a></li><li><a href=#123-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label="1.2.3 损失函数">1.2.3 损失函数</a></li></ul></li><li><a href=#13-%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84-detr-architecture aria-label="1.3 网络结构 (DETR architecture)">1.3 网络结构 (DETR architecture)</a><ul><li><a href=#131-backbone aria-label="1.3.1 Backbone">1.3.1 Backbone</a></li><li><a href=#132-transformer-encoder aria-label="1.3.2 Transformer Encoder">1.3.2 Transformer Encoder</a></li><li><a href=#133-transformer-decoder aria-label="1.3.3 Transformer Decoder">1.3.3 Transformer Decoder</a></li><li><a href=#134-ffns aria-label="1.3.4 FFNs">1.3.4 FFNs</a></li><li><a href=#135-%e8%be%85%e5%8a%a9 aria-label="1.3.5 辅助">1.3.5 辅助</a></li></ul></li><li><a href=#14-detr%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7 aria-label="1.4 DETR的局限性">1.4 DETR的局限性</a></li><li><a href=#2-dcn aria-label="2. DCN">2. DCN</a><ul><li><a href=#21%e6%99%ae%e9%80%9a%e5%8d%b7%e7%a7%af aria-label=2.1普通卷积>2.1普通卷积</a></li><li><a href=#22%e5%8f%af%e5%8f%98%e5%bd%a2%e5%8d%b7%e7%a7%af aria-label=2.2可变形卷积>2.2可变形卷积</a></li></ul></li><li><a href=#3deformabledetr aria-label=3.DeformableDETR>3.DeformableDETR</a><ul><li><a href=#31%e5%8f%af%e5%8f%98%e6%80%a7%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%a8%a1%e5%9d%97deformable-attention-module aria-label="3.1可变性注意力模块(Deformable Attention Module)">3.1可变性注意力模块(Deformable Attention Module)</a></li><li><a href=#32%e5%a4%9a%e5%b0%ba%e5%ba%a6%e5%8f%af%e5%8f%98%e6%80%a7%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%a8%a1%e5%9d%97multi-scale-deformable-attention-module aria-label="3.2多尺度可变性注意力模块(Multi-scale Deformable Attention Module)">3.2多尺度可变性注意力模块(Multi-scale Deformable Attention Module)</a></li><li><a href=#33-deformable-transformer-encoder aria-label="3.3 Deformable Transformer Encoder">3.3 Deformable Transformer Encoder</a></li><li><a href=#34-deformable-transformer-decoder aria-label="3.4 Deformable Transformer Decoder">3.4 Deformable Transformer Decoder</a></li></ul></li><li><a href=#4boundingbox%e5%9b%9e%e5%bd%92%e6%8d%9f%e5%a4%b1 aria-label=4.BoundingBox回归损失>4.BoundingBox回归损失</a><ul><li><a href=#41-l1%e6%8d%9f%e5%a4%b1 aria-label="4.1 L1损失">4.1 L1损失</a></li><li><a href=#42-l2%e6%8d%9f%e5%a4%b1 aria-label="4.2 L2损失">4.2 L2损失</a></li><li><a href=#43-smooth-l1 aria-label="4.3 Smooth L1">4.3 Smooth L1</a></li></ul></li><li><a href=#5-iou%e6%8d%9f%e5%a4%b1 aria-label="5. IoU损失">5. IoU损失</a><ul><li><a href=#51-iouintersection-over-union aria-label="5.1 IoU**(Intersection over Union)**">5.1 IoU**(Intersection over Union)**</a></li><li><a href=#52-giougeneralized-intersection-over-union aria-label="5.2 GIoU**(Generalized Intersection over Union)**">5.2 GIoU**(Generalized Intersection over Union)**</a></li></ul></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=deformabledetr>DeformableDETR<a hidden class=anchor aria-hidden=true href=#deformabledetr>#</a></h1><h1 id=1detr>1.DETR<a hidden class=anchor aria-hidden=true href=#1detr>#</a></h1><h2 id=11-简述>1.1 简述<a hidden class=anchor aria-hidden=true href=#11-简述>#</a></h2><p>Detr将目标检测任务看作是集合预测问题，在每次将所有的object都做出预测，之后与GroundTruth进行二分图匹配，每一个object预测结果都会对应一个种类与位置(当然其中包含背景种类)；并且使用transformer来构建模型，对所有的objectquery来并行预测。</p><p>相比较以往的目标检测模型，DETR主要有以下几个创新点：</p><ol><li>将目标检测任务看作是集合预测问题，并为此设计了损失函数，得到固定数量的object预测之后与真实值进行二分匹配，从而不需要以往的去重工作（如NMS）</li><li>使用transformer来并行预测与建模，不再需要先验知识(如anchor的设计)</li></ol><p>总之，detr是首个真正的端到端目标检测模型，也是将transformer运用到目标检测任务中的首次尝试。</p><h2 id=12损失函数object-detection-set-prediction-loss>1.2损失函数（Object detection set prediction loss）<a hidden class=anchor aria-hidden=true href=#12损失函数object-detection-set-prediction-loss>#</a></h2><p>DETR每次得出固定数量$N$ 的预测结果，当然$N$ 需要大于在图片中出现的物体数量，之后得出这些预测结果与真实值之间的最佳二分匹配，得出最佳匹配的方法表示如下：</p><p>其中，$y$ 代表所预测目标的真实值，当然其中包括物体的种类与位置；$\hat y={\hat y_i}<em>{i=1}^{N}$ 表示对每个objectquery得到的预测结果,$\sigma(i)$ 则表示对于第$i$ 个预测结果所匹配到的groundtruth的索引，$\ell</em>{match}(y_i,\hat y_{\sigma(i)})$ 表示二者之间的匹配成本。我们最终需要的使匹配成本最小的$\sigma$ 。当然这里可以使用匈牙利算法。</p><h3 id=121匹配成本>1.2.1匹配成本<a hidden class=anchor aria-hidden=true href=#121匹配成本>#</a></h3><p>这里的匹配成本同时考虑了物体的种类与位置，即$\hat y = (c_i,b_i)$ ,其中$c_i$表示物体种类，$b_i \in [0,1]^4$ 为包含ground box中心坐标与长、宽的向量。对于索引$\sigma(i)$ 我们将该索引对应的预测结果分类为$c_i$ 的概率表示为$p_{\sigma(i)}(c_i)$ ,将其预测的box表示为$\hat b_{\sigma (i)}$ ，因此可以将匹配成本函数定义为：$L_{match}(y_i, \hat y_{\sigma(i)}) =−1_{c_i\not=\emptyset}\hat p_{\sigma(i)}(c_i) + 1_{c_i\not=\emptyset} L_{box}(b_i, \hat b_{\sigma_i}(i))$ 。这样的过程与以往的目标检测算法中为anchor或者proposal分配真实值的工作效果相同，当然区别在与我们得到的是一一对应的匹配，不存在重复的问题。</p><h3 id=122-标记框损失bounding-box-loss>1.2.2 标记框损失(Bounding box loss)<a hidden class=anchor aria-hidden=true href=#122-标记框损失bounding-box-loss>#</a></h3><p>因为本模型是直接做出对于box的预测的，所以会遇到不同尺度下损失的相对缩放。常用的$l_1$ 损失会跟随预测框的大小产生缩放，即使这两种预测框的错误程度是相同的。而IoU损失的尺度是不变的，因此这里将$l_1$ 损失与IoU损失做线性组合，成为我们需要的boxloss，如下：</p><p>当然，其中$\lambda_{iou}$ 与$\lambda_{L_1}$ 都是超参数。</p><h3 id=123-损失函数>1.2.3 损失函数<a hidden class=anchor aria-hidden=true href=#123-损失函数>#</a></h3><p>之后就可以定义匹配的损失函数如下：</p><h2 id=13-网络结构-detr-architecture>1.3 网络结构 (DETR architecture)<a hidden class=anchor aria-hidden=true href=#13-网络结构-detr-architecture>#</a></h2><p>先上图</p><h3 id=131-backbone>1.3.1 Backbone<a hidden class=anchor aria-hidden=true href=#131-backbone>#</a></h3><p>即:使用CNN从原始三通道图像$x_{img}\in R^{3\times H_0 \times W_0}$ 抽取出特征图$f \in R^{d \times h \times w}$ 。在原文中，$C=2048$,$h,w=h_0/32,w_0/32$</p><h3 id=132-transformer-encoder>1.3.2 Transformer Encoder<a hidden class=anchor aria-hidden=true href=#132-transformer-encoder>#</a></h3><p>在编码器中，首先使用$1 \times 1$卷积核，将特征图f 的通道数从C降为d，创建特征图$z_0 \in R^{d \times h \times w}$。因为transformer的编码器需要的是序列化的输入，因此再将$z_0$展平为$d \times hw$的序列，这个时候每个像素点可以类比为序列中的一个word embedding；之后按照transformer的流程，再加入位置编码，不同的是，每一次进行注意力机制之前都需要加入一次位置编码，不同于transformer的只加入一次。</p><p>之后即可进行多头自注意力机制与前馈网络(FFN)，与transformer一致。</p><h3 id=133-transformer-decoder>1.3.3 Transformer Decoder<a hidden class=anchor aria-hidden=true href=#133-transformer-decoder>#</a></h3><p>解码器同样与transformer非常相近，输入为N个objectqurey,长度为d，这些作为query进行多头注意力与自注意力机制。不同点在于，原transformer在做预测的时候依然是串行进行的，即每次根据之前的输入预测下一个元素；而本文中则是对所有的object query并行做出预测，同时产生$N$ 个预测结果，当然这些预测结果都是各不相同的。</p><p>多次提到的object query,实际上就是一种可学习的位置编码，当然也同样在每层中的注意力机制加入。</p><p>得益于attention，我们可以同时看到整张图片，从而做出充分考虑上下文内容的预测；但是超大的感受野也为detr埋下了隐患。</p><h3 id=134-ffns>1.3.4 FFNs<a hidden class=anchor aria-hidden=true href=#134-ffns>#</a></h3><p>在decoder中，N个object query会产生N个输出，而最终的预测结果是由FFN加工完成的。对于decoder输出的embedding,FFN用一个具有ReLU激活函数和d维隐藏层的3层感知机，来根据输出得到预测框的相对于原图归一化后中心点坐标、长和宽；再通过具有softmax的线性映射将输出映射为预测类别。</p><h3 id=135-辅助>1.3.5 辅助<a hidden class=anchor aria-hidden=true href=#135-辅助>#</a></h3><p>在训练时，还用到了辅助的损失函数，来帮助模型正确输出每个种类的物体的数量。具体为在每层decoder中都使用FFN与匈牙利损失。所有的FFN都具有相同的参数。</p><h2 id=14-detr的局限性>1.4 DETR的局限性<a hidden class=anchor aria-hidden=true href=#14-detr的局限性>#</a></h2><p>detr成功得将transformer运用到目标检测领域，代替了此前在目标检测中的anchor标准和筛选处理，成为第一个完全端到端目标检测器，但是它仍然存在一些问题：</p><ol><li>由于transformer的特性(注意力机制能同时看到所有字)导致计算开销大，收敛慢</li><li>处理图像的分辨率有限制，在小目标识别方面有提升空间(但只是相对于DETR来说)</li></ol><p>基于为了改进以上两点，本文提出了DeformalbeDETR</p><h2 id=2-dcn>2. DCN<a hidden class=anchor aria-hidden=true href=#2-dcn>#</a></h2><p>接下来需要了解可变形卷积的基本思想。</p><h3 id=21普通卷积>2.1普通卷积<a hidden class=anchor aria-hidden=true href=#21普通卷积>#</a></h3><p>普通卷积运算公式如下：</p><p>其中，$p_0$代表卷积核心的坐标，$p_n$ 代表卷积核感受野中需要纳入考量的单元相对于核心的偏移量。</p><p>可入下图所示：</p><p>在普通卷积下，相对于核心的偏移都是固定的，这也限制了卷积的感受野，对于学习到较远距离元素的关系计算开销很大。</p><h3 id=22可变形卷积>2.2可变形卷积<a hidden class=anchor aria-hidden=true href=#22可变形卷积>#</a></h3><p>计算公式：</p><p>可变形卷积中进一步加入了$\triangle p_n$ ，是通过一层卷积的到的值，学习方法：</p><p>$\triangle p_n$是通过在特征图上增加一个卷积层来得到的。这个卷积核的尺寸与可变形卷积的尺寸一致。输出的offset fields有着与输入特征图一样的尺寸，通道维度2N与N个2D的offsets相关联</p><p>这里offsetfield的channel数为18（9个点的偏移量，每个量2个参数）</p><p>$\triangle p_n$代表偏移量的偏移量，如下图所示：</p><p>当然$\triangle p_n$ 通常是小数，为了计算参与卷积运算的点的值，这里会选择距编译点最近的四个点进行双线性差值，其结果参与卷积运算。</p><p>因此可变形卷积可如下表示：</p><h2 id=3deformabledetr>3.DeformableDETR<a hidden class=anchor aria-hidden=true href=#3deformabledetr>#</a></h2><p>为了解决DETR中的两个问题，该论文有以下两个创新点：</p><h3 id=31可变性注意力模块deformable-attention-module>3.1可变性注意力模块(Deformable Attention Module)<a hidden class=anchor aria-hidden=true href=#31可变性注意力模块deformable-attention-module>#</a></h3><p>针对注意力机制会处理所有位置的token,参考可变形卷积，不论特征图有多大，对于每个query只对有限的几个key应用注意力机制(在代码中这个采样的数量是4)，减少了运算开销，从而缓解收敛满和图像分辨率限制的问题。</p><p>对于特征图$x \in R^{C\times H \times W}$ ，$z_q$ 表示特征图中的一个token,$p_q$ 表示该token在特征图中的二维坐标(类比卷积)，则可变形注意力机制可以表示为：</p><p>其中，m标准多头注意力的序号；k表示采样得到的key的序号；$\Delta P_{mqk}$ 表示对于key采样的偏移，由query $z_q$ 线性映射获得；由$p_q$ 表示原本应当采样的坐标(这里和可变性卷积稍有出入)；因此$x(p_q+\Delta p_{mqk})$ 表示在特征图中采样得到的token的二维坐标（当然这里也用到了双线性差值），用该坐标位置的token对应的key(需要经过线性映射)的key与value,与query进行注意力机制，而后得到对应的注意力权重$A_{mqk}$（当然原文中$A_{mqk}$ 也是由$z_q$ 线性映射后softmax得到的，是注意力的变体，不是传统的注意力机制）；最后再乘以权重参数$W_m^{&rsquo;}$ (原文中这个是单位矩阵，费解)，最后再乘以不同注意力头分配的权重，得到注意力机制的结果。</p><h3 id=32多尺度可变性注意力模块multi-scale-deformable-attention-module>3.2多尺度可变性注意力模块(Multi-scale Deformable Attention Module)<a hidden class=anchor aria-hidden=true href=#32多尺度可变性注意力模块multi-scale-deformable-attention-module>#</a></h3><p>以上提到的注意力机制可以很自然地应用于多尺度特征图中，公式如下：</p><p>其中，m表示注意力头序号，k表示采样点序号，l表示特征图的尺度等级。</p><p>特别需要说明的是，为了在不同尺度(当然这里可以理解为分辨率)下应用这个可变性注意力机制，我们需要进行采样时坐标的处理，因此这里将采样点二维坐标进行归一化$\hat p_q \in [0,1]^2$,则在不同特征图尺度下可以找到同一位置的点。之后为了在不同尺度下取的指定位置的$z_q$ 值，再通过$\phi_l()$ 函数将$p_q$ 映射回原尺度得到坐标。</p><h3 id=33-deformable-transformer-encoder>3.3 Deformable Transformer Encoder<a hidden class=anchor aria-hidden=true href=#33-deformable-transformer-encoder>#</a></h3><p>在编码器中，从ResNet的$C_3$ 到$C_5$ 阶段的输出中提取四个等级尺度$(L=4)$的特征图，$C_l$ 阶段的特征图比原图低 $2^l$ 倍，最低分辨率特征图是$C_6$,也就是$C_5$ 再通过步长为2的$3\times3$ 卷积得到。编码器的输出是和输入相同分辨率的多尺度特征图，key和query元素都是多尺度特征图中的像素。对于每一个query,参考点就是它本身。</p><p>为了表示尺度信息，类比transformer中的position embedding,在这里也加入了scale-level embedding,这个embedding是通过网络训练而成的。</p><h3 id=34-deformable-transformer-decoder>3.4 Deformable Transformer Decoder<a hidden class=anchor aria-hidden=true href=#34-deformable-transformer-decoder>#</a></h3><p><strong>实际是在学习检测框的特征</strong></p><p>解码器中有交叉注意力与自注意力，两种注意力的query都是object。在交叉注意力中，key来自编码器，object query从特征图中抽取特征。在自注意中，object query互相作用，这时key为object query本身。当然在这里只在交叉注意力之中，应用deformable attention，自注意力不变。对于每一个object query,参考点的坐标是根据object embedding通过一层可学习的线性映射得到的。</p><p>因为多尺度可变形卷积是依据参考点来提取特征的，所以我们用检测头预测boundingbox作为相对于参考点的坐标偏移，参考点作为最初预测框的中心。因此，解码器的注意力机制会与预测框有很大的相关性。</p><h2 id=4boundingbox回归损失>4.BoundingBox回归损失<a hidden class=anchor aria-hidden=true href=#4boundingbox回归损失>#</a></h2><h3 id=41-l1损失>4.1 L1损失<a hidden class=anchor aria-hidden=true href=#41-l1损失>#</a></h3><p>L1损失也即平均绝对误差(MAE)，顾名思义公式如下：</p><p>$$
L1(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$</p><p>其曲线分布如下：</p><p>特点：</p><ul><li>导数为常量，不会有梯度爆炸问题；</li><li>对于离群点的惩罚是是固定的；</li></ul><p>但是：</p><ul><li>在0处不可导</li><li>正因为导数为常量，对于较小的损失值也具有较大的梯度，缺乏自适应性</li></ul><h3 id=42-l2损失>4.2 L2损失<a hidden class=anchor aria-hidden=true href=#42-l2损失>#</a></h3><p>也称均方误差(MSE)，一目了然公式如下：</p><p>$$
L2(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$</p><p>其曲线如下：</p><p>特点：</p><ul><li>出处连续，处处可导；</li><li>并且随着误差值减小，梯度也会减小，利于收敛</li></ul><p>但是：</p><ul><li>当$y-f(x)$ 差值大于1时，平方项会放大误差，对离群点惩罚比较大</li></ul><h3 id=43-smooth-l1>4.3 Smooth L1<a hidden class=anchor aria-hidden=true href=#43-smooth-l1>#</a></h3><p>即以上两种损失的联合，在误差小于1时为L2损失，大于1时为L1损失</p><p>公式如下：</p><p>$$
\text{SmoothL1}(y, \hat{y}) = \begin{cases} 0.5 * (y - \hat{y})^2 & \text{if } |y - \hat{y}| &lt; 1 \ |y - \hat{y}| - 0.5 & \text{otherwise} \end{cases}
$$</p><p>曲线如下：</p><p>特点：</p><ul><li>当误差过大，梯度不会过大</li><li>当误差比较小，梯度也会相应减小</li></ul><p>实际上就是联合了以上两种函数的优点。</p><h2 id=5-iou损失>5. IoU损失<a hidden class=anchor aria-hidden=true href=#5-iou损失>#</a></h2><h3 id=51-iouintersection-over-union>5.1 IoU**(Intersection over Union)**<a hidden class=anchor aria-hidden=true href=#51-iouintersection-over-union>#</a></h3><p>用于衡量模型预测的区域与真实区域之间的重叠程度</p><p>即计算两个区域的交集面积与它们的并集面积之比。</p><p>IoU损失的计算公式如下所示：</p><p>$$
IoU = \frac{{\text{Intersection Area}}}{{\text{Union Area}}} = \frac{{\text{TP}}}{{\text{TP + FP + FN}}}
$$</p><p>其中：</p><ul><li>TP（True Positive）：真正例，模型正确预测的正样本数量。</li><li>FP（False Positive）：假正例，模型错误预测的正样本数量。</li><li>FN（False Negative）：假负例，模型未能正确预测的正样本数量。</li></ul><p>实际上也可简单理解为交并比：</p><p>$$
IoU = \frac{{|A \cap B|}}{{|A \cup B|}}
$$</p><p>IoU损失越小，表示模型的预测与真实情况的重叠程度越大。</p><p>由于IoU为比值形式，所以它具有一个很好的特性是，对<strong>尺度(scale)不敏感</strong></p><h3 id=52-giougeneralized-intersection-over-union>5.2 GIoU**(Generalized Intersection over Union)**<a hidden class=anchor aria-hidden=true href=#52-giougeneralized-intersection-over-union>#</a></h3><p>由于BBox中的回归损失对scale比较敏感，并且其优化与IoU损失的优化不是等价的，因此对IoU损失进行改进，在计算目标检测框的重叠程度时考虑了检测框的整体位置和大小关系，不仅仅是交并关系，从而设置为回归损失：</p><p>$$
GIoU = IoU - \frac{{|C- (A \cup B)|}}{{|C|}}
$$</p><p>其中：</p><ul><li>$|C - (A \cup B)|$ 表示外部包围框 C 减去 A和 B的并集的大小（即不相交的部分的大小）。</li><li>|C| 示外部包围框 C 的大小，同时包含了预测框和真实框的最小框的面积。</li></ul><p>GIoU 将 IoU 的值减去了外部不相交部分与外部包围框的比例，不仅关注重叠区域，还关注非重叠区域，从而考虑了检测框的整体位置关系,能更好的反映两者的重合度。使得 GIoU 更能够准确地反映检测框的匹配情况。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://zqlqy.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://zqlqy.github.io/tags/transformer/>Transformer</a></li><li><a href=https://zqlqy.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/>目标检测</a></li></ul><nav class=paginav><a class=next href=https://zqlqy.github.io/posts/diffusion%E7%AE%80%E4%BB%8B/><span class=title>Next »</span><br><span>Diffusion简介</span></a></nav></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>💬评论</span><hr></div><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.32/twikoo.all.min.js></script><script>twikoo.init({envId:"https://logcomment-zqls-projects-7b4d5b80.vercel.app",el:"#tcomment",lang:"zh-CN",region:"",path:window.TWIKOO_MAGIC_PATH||window.location.pathname})</script></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://zqlqy.github.io/>ZQLog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>